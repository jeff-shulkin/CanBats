{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is optimized to run on a gpu instance of Amazon Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# All of these modules are either included in the code base\n",
    "# or provided by default on Amazon Sagemaker. \n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from db import NABat_DB\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        ReduceLROnPlateau)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tensorflow version and whether we have access to a gpu.\n",
    "print(tf.__version__)\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set the random seed for repeatability.\n",
    "seed = 546\n",
    "random.seed(seed)\n",
    "\n",
    "# Create a place to store our trained models.\n",
    "try:\n",
    "    os.mkdir('models')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = NABat_DB()\n",
    "\n",
    "# Point to a directory of species codes containing .wav files. \n",
    "# Example \"../v1.1.0/data/wav/ANPA/p163_g89522_f28390444.wav\"\n",
    "directory = './WavesW'\n",
    "\n",
    "\n",
    "# Make sure there are at least 3 example files for each class we want to include.\n",
    "sample_classes = []\n",
    "x = [c.split('/')[-1] for c in glob.glob('{}/*'.format(directory), recursive=True)]\n",
    "for c in x:\n",
    "    size = len(glob.glob('{}/{}/*'.format(directory, c), recursive=True))\n",
    "    if size > 40:\n",
    "        sample_classes.append(c)\n",
    "        \n",
    "# Alphibitize.\n",
    "sample_classes.sort()\n",
    "\n",
    "# Print the classes we will include.\n",
    "pprint.pprint(sample_classes)\n",
    "\n",
    "# Set the database flag describing which species classes will be considered \n",
    "# in this model training run.\n",
    "db.insert('update species set available = 0') \n",
    "for s in sample_classes:\n",
    "    db.insert('update species set available = 1 where species_code = ?',(s,)) \n",
    "\n",
    "db.conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep this connection open for the life of this model training run.\n",
    "db = NABat_DB()\n",
    "\n",
    "# Return a list of files belonging to a dataset. 'train', 'test', or 'validate'.\n",
    "def get_files(draw):\n",
    "    return db.query('select id, name, grts_id from file where draw = ? and grts_id != 0 order by id',(draw,))\n",
    "\n",
    "# Return list of bat pulses that originated from a given recording file, by file id.\n",
    "def get_pulses(file_id):\n",
    "    return db.query('select * from pulse where file_id = ? order by id',(file_id,))\n",
    "\n",
    "# Yield a spectrogram image and the class it belongs to. \n",
    "def image_generator(draw):\n",
    "    try:\n",
    "        draw = draw.decode(\"utf-8\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get list of files.\n",
    "    files = get_files(draw)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        species = f.name.split('/')[-2]\n",
    "        if species in sample_classes:\n",
    "            species_id = sample_classes.index(species)\n",
    "            \n",
    "            # Get a list of pulses (and path to associated spectrogram image on disk) belonging to file.\n",
    "            metadata = get_pulses(f.id)\n",
    "            \n",
    "            for i, m in enumerate(metadata):\n",
    "                # Normalize the image so that each pixel value\n",
    "                # is scaled between 0 and 1.\n",
    "                image = Image.open(m.path)\n",
    "                img = np.array(image)\n",
    "                img = img[..., :3].astype('float32')\n",
    "                img /= 255.0\n",
    "                image.close()\n",
    "                yield {\"input_1\": img}, species_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the image generator function.\n",
    "gen = image_generator('test')\n",
    "print(next(gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the batch size for the network.\n",
    "batch_size = 32\n",
    "\n",
    "# Create a training dataset.\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    image_generator, args=['train'],\n",
    "    output_types=({\"input_1\": tf.float16}, tf.int32),\n",
    "    output_shapes=({\"input_1\": (100,100,3)}, () )                                      \n",
    "    ).batch(batch_size).prefetch(1000)\n",
    "\n",
    "# Create a validation dataset.\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    image_generator, args=['validate'],\n",
    "    output_types=({\"input_1\": tf.float16}, tf.int32),\n",
    "    output_shapes=({\"input_1\": (100,100,3)}, () )                                      \n",
    "    ).batch(batch_size).prefetch(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the metadata for the current model run.\n",
    "with open('models/training_history_{}.p'.format('0'), 'wb') as fp:\n",
    "\n",
    "    # Define model inputs.\n",
    "    inputs = layers.Input(shape=(100,100,3))\n",
    "\n",
    "    # Define network shape.\n",
    "    w = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    w = layers.Dropout(0.2)(w)\n",
    "    w = layers.MaxPooling2D(padding='same')(w)\n",
    "    w = layers.Conv2D(64, 3, activation='relu', padding='same')(w)\n",
    "    w = layers.Conv2D(64, 3, activation='relu', padding='same')(w)\n",
    "    w = layers.Dropout(0.2)(w)\n",
    "    w = layers.MaxPooling2D(padding='same')(w)\n",
    "    w = layers.Conv2D(128, 3, activation='relu', padding='same')(w)\n",
    "    w = layers.Conv2D(128, 3, activation='relu', padding='same')(w)\n",
    "    w = layers.Conv2D(128, 3, activation='relu', padding='same')(w)\n",
    "    w = layers.Dropout(0.2)(w)\n",
    "    w = layers.MaxPooling2D(padding='same')(w)\n",
    "    w = layers.Flatten()(w)\n",
    "    w = layers.Dropout(0.4)(w)\n",
    "    w = layers.Dense(256, activation='relu')(w)\n",
    "    w = layers.Dropout(0.4)(w)\n",
    "    w = layers.Dense(256, activation='relu')(w)\n",
    "    w = layers.Dropout(0.4)(w)\n",
    "    w = layers.Dense(256, activation=\"relu\")(w)\n",
    "    w = layers.Dense(len(sample_classes), activation=\"softmax\")(w)\n",
    "   \n",
    "    model = Model(inputs=inputs, outputs=w)\n",
    "    \n",
    "    # Print and plot network.\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    # Set the hyperparameters for this model run.\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9,\n",
    "                                        beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "                                        name='Adam'\n",
    "                                        ),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    # Define the early stopping criteria.\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, restore_best_weights=True)\n",
    "\n",
    "    # Start the training. This will produce a preliminary (course) model.\n",
    "    h = model.fit(train_dataset,\n",
    "                validation_data=validation_dataset,\n",
    "                epochs=25,\n",
    "                verbose=1,\n",
    "                callbacks=[es]\n",
    "        )\n",
    "\n",
    "    # Save the metadata, model, and weights to disk.\n",
    "    model.save('models/m-{}'.format('0'))\n",
    "    pickle.dump((h.history, sample_classes), fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reopen the same model for further training.\n",
    "# Here we reduce the learning rate hyperparameter by a factor of 100\n",
    "# to fine-tune the model.\n",
    "\n",
    "with open('models/training_history_{}.p'.format('1'), 'wb') as fp:\n",
    "\n",
    "    model = keras.models.load_model('models/m-{}'.format('0'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.9,\n",
    "                                        beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "                                        name='Adam'\n",
    "                                        ),\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2, restore_best_weights=True)\n",
    "\n",
    "    h = model.fit(train_dataset,\n",
    "                validation_data=validation_dataset,\n",
    "                epochs=50,\n",
    "                verbose=1,\n",
    "                callbacks=[es]\n",
    "        )\n",
    "\n",
    "    model.save('models/m-{}'.format('1'))\n",
    "    pickle.dump((h.history, sample_classes), fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional, plot the accuracy and loss curves of the training and validation sets.\n",
    "def plot_training():\n",
    "    \n",
    "    epochs_range = range(40)\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    for i in range(0,99,1):\n",
    "        try:\n",
    "            with open('models/training_history_{}.p'.format(i), 'rb') as fp:\n",
    "                m = pickle.load(fp)[0]\n",
    "                acc = m['accuracy'] + ([0] * (epochs_range[-1] - len(m['accuracy'])))\n",
    "                val_acc = m['val_accuracy'] + ([float('nan')] * (epochs_range[-1] - len(m['val_accuracy'])))\n",
    "                \n",
    "                plt.plot(epochs_range[:-1], acc, label='Training Accuracy {}'.format(i))\n",
    "                plt.plot(epochs_range[:-1], val_acc, label='Validation Accuracy {}'.format(i))\n",
    "    \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "        \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    for i in range(0,99,1):\n",
    "        try:\n",
    "            with open('models/training_history_{}.p'.format(i), 'rb') as fp:\n",
    "                m = pickle.load(fp)[0]\n",
    "                loss = m['loss'] + ([0] * (epochs_range[-1] - len(m['loss'])))\n",
    "                val_loss = m['val_loss']\n",
    "                val_loss += ([float('nan')] * (epochs_range[-1] - len(m['val_loss'])))\n",
    "                \n",
    "                plt.plot(epochs_range[:-1], loss, label='Training Loss {}'.format(i))\n",
    "                plt.plot(epochs_range[:-1], val_loss, label='Validation Loss {}'.format(i))\n",
    "    \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get available information for species.\n",
    "species = db.query('select * from species;')\n",
    "\n",
    "# Get the index of the species in the model output from the species code.\n",
    "def get_species_index(species_code):\n",
    "    for i, s in enumerate(species):\n",
    "        if s.species_code == species_code:\n",
    "            return i\n",
    "\n",
    "# Define a new test generator that will represent the pulse id instead\n",
    "# of the species id, since we want to predict the latter.\n",
    "def test_generator(draw):\n",
    "    try:\n",
    "        draw = draw.decode(\"utf-8\")\n",
    "    except:\n",
    "        pass\n",
    "    files = get_files(draw)\n",
    "    for f in files:\n",
    "        species = f.name.split('/')[-2]\n",
    "        s_index = get_species_index(species)\n",
    "        if species in sample_classes:\n",
    "            species_id = sample_classes.index(species)\n",
    "            metadata = get_pulses(f.id)\n",
    "            for i, m in enumerate(metadata):\n",
    "                image = Image.open(m.path)\n",
    "                img = np.array(image)\n",
    "                img = img[..., :3].astype('float32')\n",
    "                img /= 255.0\n",
    "                image.close()\n",
    "                yield {\"input_1\": img}, m.id\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the tuned model.\n",
    "m = '0'\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "\n",
    "# Insert a set of predictions into the database.\n",
    "def insert(data):\n",
    "    db.conn.executemany(\n",
    "        \"insert into prediction (model_name, pulse_id, confidence, species_id) values (?,?,?,?);\", data)\n",
    "    db.conn.execute('commit;')\n",
    "\n",
    "\n",
    "# Given a species code, return a numeric id.\n",
    "def get_manual_id(species_code):\n",
    "    for s in species:\n",
    "        if s.species_code == species_code:\n",
    "            return s.id\n",
    "\n",
    "# Load the tuned model from disk.\n",
    "model = keras.models.load_model('models/m-{}'.format(m))\n",
    "\n",
    "\n",
    "# Provide a prediction for each pulse in the test set.\n",
    "to_predict1 = []\n",
    "ids = []\n",
    "data = []\n",
    "count = 0\n",
    "for p in test_generator('test'):     \n",
    "    count += 1\n",
    "    to_predict1.append(p[0]['input_1'])\n",
    "    ids.append(p[1])\n",
    "    \n",
    "    # Batch the predictions into groupes of 1024.\n",
    "    if count != 0 and (count % 1024 == 0 ):\n",
    "        predictions = model.predict(np.concatenate([to_predict1],axis=0),batch_size=1024)\n",
    "        for x, prediction in enumerate(predictions):\n",
    "            for i, c in enumerate(prediction):\n",
    "                data.append((str(m), ids[x], float(c), get_manual_id(sample_classes[i])))\n",
    "        \n",
    "        insert(data)\n",
    "        to_predict1 = []\n",
    "        data = []\n",
    "        ids = []\n",
    "        \n",
    "        # Clean up\n",
    "        gc.collect()\n",
    "        \n",
    "        # Report progress.\n",
    "        print('{}'.format(int(count)))\n",
    "\n",
    "# Predict the remaining < 1024 predictions not batched in prior step.\n",
    "predictions = model.predict(np.concatenate([to_predict1],axis=0),batch_size=len(ids))\n",
    "for x, prediction in enumerate(predictions):\n",
    "    for i, c in enumerate(prediction):\n",
    "        data.append((str(m), ids[x], float(c), get_manual_id(sample_classes[i])))\n",
    "insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.query(\"select count(*) as count_predictions from prediction where model_name = 0\"))\n",
    "print(db.query(\"select count(*) as count_predictions from prediction where model_name = 1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
